# -*- coding: utf-8 -*-
"""數據科學薪資ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hJ17xtg9UJizy6RgFp6P6A2iRa7lipxq

# 安裝套件
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip uninstall scikit-learn pycaret -y -qq
# %pip install scikit-learn pycaret -qq
# %pip install association-metrics -qq
# %pip install --upgrade association-metrics -qq

"""#載入套件"""

import sklearn
from pycaret.classification import *
import os
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
from wordcloud import WordCloud, STOPWORDS, get_single_color_func
import association_metrics as am
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from scipy.stats import chi2_contingency
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from tensorflow.keras.regularizers import l2

"""# 關掉科學記號"""

np.set_printoptions(suppress=True)
pd.set_option('display.float_format',lambda x : '%.3f' % x)

"""# 載入資料"""

!gdown --fuzzy 'https://drive.google.com/file/d/1oPufBbrUQiU8CNu8NkLr-cK4fxew-pDO/view?usp=sharing'
data = pd.read_csv("/content/Latest_Data_Science_Salaries.csv ")

"""## 資料複製"""

df = data.copy()

Columns = df.columns
colors = sns.color_palette("Set2") #設置顏色

df["Company Location"].unique()

"""# EDA

## 職稱數量長條圖與文字雲
"""

# 定義自訂的顏色函數，用於根據每個單詞的 y 位置來計算顏色
def gradient_color_func(word, font_size, position, orientation, random_state=None, **kwargs):
    # 使用 position[1]（y 坐標）來設置顏色的色調值。hue 的範圍是 0 到 360 度。
    hue = int(360 + (180 * position[1] / plt.gcf().get_size_inches()[1]))  # 根據 y 位置來變換顏色
    return f"hsl({hue}, 100%, 25%)"  # 使用 HSL 顏色模式設定顏色

text = " ".join(title for title in df["Job Title"])
word_cloud = WordCloud(collocations = False, color_func=gradient_color_func, background_color = 'white', width=1600, height=800, random_state=2024).generate(text)
plt.figure(figsize = (10, 5),dpi=200)
plt.imshow(word_cloud, interpolation = 'bilinear')
plt.axis("off")
plt.title('WorldCloud for Job Titles\n', fontsize = 12, color = "#FF0000",fontweight='bold')
plt.show()

# 提取 "Job Title" 欄位中出現次數最多的前 20 種職稱
top_20_job_title = df["Job Title"].value_counts()[:20]
top_20_job_title

plt.figure(figsize=(16, 7))

# 將前 20 名職稱按出現次數升序排列並繪製水平條形圖
top_20_job_title.sort_values(ascending=True).plot(
    kind='barh',
    color=sns.color_palette('tab20'),
    edgecolor='black'
)

plt.ylabel('Job Titles', fontsize=16)
plt.xlabel('\nNumber of Occurrences', fontsize=16)
plt.title('Top 20 Job Titles\n', fontsize=16)
plt.xticks(rotation=0, ha='center', fontsize=16)

plt.tight_layout()

plt.show()

"""## 國家縮寫"""

### 將單詞的國家名稱簡寫 ###
def abbreviate_country(country, change_before, change_after):
    if ' ' in country: # 如果國家名稱包含空格，代表它由多個單詞組成
        change_before.append(country)
        abbreviation = ''.join(word[0] for word in country.split()).upper() # 將國家名稱的每個單詞取首字母並轉為大寫，拼接為縮寫
        change_after.append(abbreviation)
        return abbreviation
    return country # 如果國家名稱不包含空格，直接返回原值

change_before = []
change_after = []

df['Company Location_abbreviation'] = df['Company Location'].apply(
    lambda x: abbreviate_country(x, change_before, change_after)
)

print("變換前", list(set(change_before)))  # 使用 set 去重
print("變換後", list(set(change_after)))

change_before = []
change_after = []

df['Employee Residence_abbreviation'] = df['Employee Residence'].apply(
    lambda x: abbreviate_country(x, change_before, change_after)
)

print("變換前", list(set(change_before)))  # 使用 set 去重
print("變換後", list(set(change_after)))

"""## 變數與薪資的盒鬚圖"""

import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(2, 2, figsize=(18, 12))

# 第一張圖：根據 Employment Type 的薪資比較
sns.boxplot(x='Employment Type', y='Salary in USD', palette=colors, data=df, ax=axes[0,0])
axes[0,0].set_xlabel('\nEmployment Type', fontsize=12, weight='bold')
axes[0,0].set_ylabel('Salary in USD\n', fontsize=12, weight='bold')
axes[0,0].set_title('Salary Comparison by Employment Type\n', fontsize=12, weight='bold')

# 第二張圖：根據 Experience Level 的薪資比較
sns.boxplot(x='Experience Level', y='Salary in USD', palette=colors, data=df, ax=axes[0,1])
axes[0,1].set_xlabel('\nExperience Level', fontsize=12, weight='bold')
axes[0,1].set_ylabel('')
axes[0,1].set_title('Salary Comparison by Experience Level\n', fontsize=12, weight='bold')

# 第三張圖：根據 Company Size 的薪資比較
sns.boxplot(x = 'Company Size', y = 'Salary in USD', palette=colors, data=df, ax=axes[1,0])
axes[1,0].set_xlabel('\nCompany Size', fontsize=12, weight='bold')
axes[1,0].set_ylabel('')
axes[1,0].set_title('Salary Comparison by Company Size\n', fontsize=12, weight='bold')

# 第四張圖: 根據 Year 的薪資比較
sns.boxplot(x='Year', y='Salary in USD', palette=colors, data=df, ax=axes[1,1])
axes[1,1].set_xlabel('\nYear', fontsize=12, weight='bold')
axes[1,1].set_ylabel('')
axes[1,1].set_title('Salary Comparison by Year\n', fontsize=12, weight='bold')

plt.tight_layout()
plt.show()

"""## Salary Trend Over Time by Company Size"""

plt.figure(figsize=(10, 6))

# 使用 seaborn 繪製折線圖，分析公司規模對薪資隨時間變化的影響
p = sns.lineplot(
    data=df,
    x='Year',
    y='Salary in USD',
    hue='Company Size',
    marker='o'
)

plt.xlabel('Year', fontsize=12, fontweight='bold')
plt.ylabel('Salary in USD', fontsize=12, fontweight='bold')
plt.legend(title='Company Size', title_fontsize=10, fontsize=10, loc='upper left')
plt.title('Salary Trend Over Time by Company Size', fontsize=14, fontweight='bold')
p.set_facecolor("#f4f4f4")
p.grid(False)
plt.show()

"""## Salary Trend Over Time by Experience Level"""

plt.figure(figsize=(10, 6))

# 使用 seaborn 繪製折線圖，分析不同工作經驗級別的薪資隨年份變化的趨勢
p = sns.lineplot(
    data=df,
    x='Year',
    y='Salary in USD',
    hue='Experience Level',
    marker='o'
)

plt.xlabel('Year', fontsize=12, fontweight='bold')
plt.ylabel('Salary in USD', fontsize=12, fontweight='bold')
plt.legend(title='Experience Level', title_fontsize=10, fontsize=10, loc='upper right')
plt.title('Salary Trend Over Time by Experience Level', fontsize=14, fontweight='bold')
p.set_facecolor("#f4f4f4")
p.grid(False)
plt.show()

"""## 公司所在地與員工居住地文字雲"""

# 將 'Company Location_abbreviation' 中的所有縮寫合併為一個字符串
text = " ".join(title for title in df['Company Location_abbreviation'])

# 創建詞雲物件，並設置相應參數
word_cloud = WordCloud(
    collocations=False,  # 不顯示詞組（只顯示單個單詞）
    color_func=gradient_color_func,
    background_color='white',
    width=1600,
    height=800,
    random_state=2024
).generate(text)

plt.figure(figsize=(10, 5), dpi=200)
plt.imshow(word_cloud, interpolation='bilinear')  # 顯示詞雲，使用雙線性插值方法使顯示更平滑
plt.axis("off")
plt.title('WorldCloud for Company Location\n', fontsize=12, color="#FF0000", fontweight='bold')
plt.show()

text = " ".join(title for title in df['Employee Residence_abbreviation'])

# 創建詞雲物件，並設置相應參數
word_cloud = WordCloud(
    collocations=False,  # 不顯示詞組（只顯示單個單詞）
    color_func=gradient_color_func,
    background_color='white',
    width=1600,
    height=800,
    random_state=2000
).generate(text)

plt.figure(figsize=(10, 5), dpi=200)
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.title('WorldCloud for Employee Residence\n', fontsize=12, color="#FF0000", fontweight='bold')
plt.show()

"""## 公司所在地與員工居住地圓餅圖"""

fig, axes = plt.subplots(1, 2, figsize=(18, 6))

top_2_counts_location = df["Company Location"].value_counts()[0:2] # 提取 "Company Location" 欄中出現次數最多的前 2 名
others_count_location = df["Company Location"].value_counts()[2:].sum() # 計算其他地點的總數（不在前 2 名的地點）
counts_location = pd.concat([top_2_counts_location, pd.Series({'Others': others_count_location})]) # 合併前 2 名地點的次數與 "Others"（其他地點）的計數，形成新的數據

# 繪製 "Company Location" 的餅圖
counts_location.plot(
    kind='pie',
    fontsize=12,
    colors=colors,
    autopct='%1.1f%%',
    ax=axes[0],
    legend=False
)

axes[0].set_xlabel('Company Location', weight="bold", color="#2F0F5D", fontsize=14, labelpad=20)
axes[0].axis('equal')

top_2_counts_residence = df["Employee Residence"].value_counts()[0:2] # 提取 "Employee Residence" 欄位中出現次數最多的前 2 名
others_count_residence = df["Employee Residence"].value_counts()[2:].sum() # 計算其他地點的總數（不在前 2 名的地點）
counts_residence = pd.concat([top_2_counts_residence, pd.Series({'Others': others_count_residence})]) # 合併前 2 名居住地的計數與 "其他地點" 的總數，形成用於餅圖的數據

# 繪製 "Employee Residence" 的餅圖
counts_residence.plot(
    kind='pie',
    fontsize=12,
    colors=colors,
    autopct='%1.1f%%',
    ax=axes[1],
    legend=False
)

axes[1].set_xlabel('Employee Residence', weight="bold", color="#2F0F5D", fontsize=14, labelpad=20)
axes[1].axis('equal')
plt.show()

"""# 資料前處理"""

#資料挑選(公司地點在美國且工作職稱出現超過20個)
Top_Job_title = list(df["Job Title"].value_counts()[df["Job Title"].value_counts() >= 20].index)
df = df[df["Job Title"].isin(Top_Job_title)].reset_index(drop = True)
df = df[df["Company Location"] == "United States"].reset_index(drop = True)

#將薪水分組(因為我們其他欄位都是類別資料)
df["Cluster"] = 0
for i in range(df.shape[0]):
  if df["Salary in USD"].iloc[i] > np.percentile(df["Salary in USD"],50):
    df["Cluster"][i] = 1
#刪除後續用不到的欄位
df.drop((['Salary in USD', 'Company Location_abbreviation', 'Expertise Level', 'Salary', 'Salary Currency', 'Company Location']), axis = 1, inplace = True)
#相同工作資料合併
df['Job Title'] = df['Job Title'].replace("ML Engineer", "Machine Learning Engineer")

#抓取工作職稱依照空格段詞
text = " ".join(title for title in df["Job Title"])
word_cloud = WordCloud(collocations=False, background_color='white', width=1600, height=800, random_state=2024).generate(text)
unique_words = word_cloud.words_.keys()
for word in unique_words:
    df[word] = df['Job Title'].apply(lambda x: 1 if word in x.split() else 0)

#將相似詞合併
df['Analyst'] = df['Analyst'] + df['Analytics']
df['Scientist'] = df['Scientist'] + df['Science']

#將資料拆分
X = df
y = df["Cluster"]  # 目標
X_train, X_test, y_train, y_test = train_test_split(X, y.to_frame(), test_size=0.2, random_state=42)

#cramers_v 類別相關係數
columns_except_Salary = X_train.astype('category')
merged_df = pd.concat([columns_except_Salary], axis=1)
cramers_v = am.CramersV(merged_df)
cfit = cramers_v.fit().round(2)

#將craners畫成相關係數圖
fig, ax = plt.subplots(figsize=(60, 60))
cax = ax.imshow(cfit, interpolation='nearest', cmap='Blues', vmin=0, vmax=1)
ax.set_xticks(ticks = range(len(cfit.columns)),
              labels = cfit.columns)
ax.set_yticks(ticks = range(len(cfit.columns)),
              labels = cfit.columns)
ax.tick_params(axis="x", labelsize=14, labelrotation=90)
ax.tick_params(axis="y", labelsize=14, labelrotation=0)
fig.colorbar(cax).ax.tick_params(labelsize=14)
for (i, j), t in np.ndenumerate(cfit):
    ax.annotate(f"{t:.2f}",
                xy=(j, i),
                va="center",
                ha="center",
                fontsize=12,
                color="black",
                )
ax.set_title('Cramér\'s V Heatmap', fontsize=22, fontweight='bold')
plt.tight_layout()
plt.show()

significant_clusters = list(cfit["Cluster"][cfit["Cluster"] > 0.1].index.drop(["Job Title","Cluster"]))
X_train_transformed = X_train[significant_clusters]
X_test_transformed = X_test[significant_clusters]
# 如果需要，可以查看轉換後的數據
print("Transformed X_train:")
print(X_train_transformed.head())
print("y_train with Clusters:")
print(y_train.head())
#X_train_transformed = X_train
#X_test_transformed = X_test

"""#建立模型"""

X_train_transformed['Cluster'] = y_train['Cluster']

X_train_transformed["Employee Residence_abbreviation"].unique()

categorical_model = setup(data = X_train_transformed,
        train_size = 0.8,
        target = 'Cluster',
        max_encoding_ohe = 10,
        session_id = 2024)

compare_models(fold = 5 , sort = 'Accuracy')

"""## Ada Boost Classifier Model"""

ada1 = create_model("ada")
test_predictions = predict_model(ada1, data=X_test_transformed)
y_test_pred = test_predictions["prediction_label"]
accuracy = accuracy_score(y_test, y_test_pred)
print(f"測試集準確率: {accuracy:.2f}")
f1 = f1_score(y_test, y_test_pred, average="weighted")
print(f"F1-Score on Test Data: {f1}")
y_test_prediction_score = test_predictions["prediction_score"]
auc = roc_auc_score(y_test, y_test_prediction_score)
print(f"獨立測試集 AUC: {auc:.2f}")

evaluate_model(ada1)

"""## Light Gradient Boosting Machine"""

lightgbm = create_model("lightgbm")
test_predictions = predict_model(lightgbm, data=X_test_transformed)
y_test_pred = test_predictions["prediction_label"]
accuracy = accuracy_score(y_test, y_test_pred)
print(f"測試集準確率: {accuracy:.2f}")
f1 = f1_score(y_test, y_test_pred, average="weighted")
print(f"F1-Score on Test Data: {f1}")
y_test_prediction_score = test_predictions["prediction_score"]
auc = roc_auc_score(y_test, y_test_prediction_score)
print(f"測試集 AUC: {auc:.2f}")

interpret_model(lightgbm ,plot = 'summary')
evaluate_model(lightgbm)

"""## Extreme Gradient Boosting"""

xgboost = create_model("xgboost")
test_predictions = predict_model(xgboost, data=X_test_transformed)
y_test_pred = test_predictions["prediction_label"]
accuracy = accuracy_score(y_test, y_test_pred)
print(f"測試集準確率: {accuracy:.2f}")
f1 = f1_score(y_test, y_test_pred, average="weighted")
print(f"F1-Score on Test Data: {f1}")
y_test_prediction_score = test_predictions["prediction_score"]
auc = roc_auc_score(y_test, y_test_prediction_score)
print(f"測試集 AUC: {auc:.2f}")

interpret_model(xgboost ,plot = 'summary')
evaluate_model(xgboost)

"""## DNN"""

X_train = get_config('X_train_transformed')
y_train = get_config('y_train_transformed')
X_test = get_config('X_test_transformed')
y_test = get_config('y_test_transformed')
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = Sequential([
    Dense(512, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.3),
    Dense(218, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.3),
    Dense(218, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.3),
    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer=Adam(learning_rate=0.001),
              loss='binary_crossentropy',  # 二元分類損失函數
              metrics=['accuracy'])

model.summary()

# 開始訓練
history = model.fit(X_train, y_train,
                    validation_split=0.2,  # 從訓練集中劃分 20% 作為驗證集
                    epochs=50,
                    batch_size=32,
                    verbose=1)

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Loss Curve')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

# 測試模型
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")
y_pred = model.predict(X_test)
y_pred_rounded = np.round(y_pred)
f1 = f1_score(y_test, y_pred_rounded)
print("F1-Score on Test Data:", f1)
auc = roc_auc_score(y_test, y_pred)
print(f"Test AUC: {auc:.2f}")